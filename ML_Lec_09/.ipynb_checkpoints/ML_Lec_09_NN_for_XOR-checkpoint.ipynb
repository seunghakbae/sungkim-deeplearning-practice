{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](xor.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(tf.random_normal([2,1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = tf.sigmoid(tf.matmul(X,W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.80746025 [[-1.413822 ]\n",
      " [ 1.1811924]]\n",
      "100 0.72565424 [[-0.71554357]\n",
      " [ 0.7417979 ]]\n",
      "200 0.70286477 [[-0.38410062]\n",
      " [ 0.406724  ]]\n",
      "300 0.6959609 [[-0.20452784]\n",
      " [ 0.21991992]]\n",
      "400 0.6939541 [[-0.10832795]\n",
      " [ 0.11872419]]\n",
      "500 0.693378 [[-0.05716146]\n",
      " [ 0.06417964]]\n",
      "600 0.6932132 [[-0.03004595]\n",
      " [ 0.03478318]]\n",
      "700 0.69316614 [[-0.01571808]\n",
      " [ 0.01891556]]\n",
      "800 0.69315267 [[-0.00817186]\n",
      " [ 0.01033003]]\n",
      "900 0.69314873 [[-0.00421363]\n",
      " [ 0.00567033]]\n",
      "\n",
      " Hypothesis :  [[0.49985364]\n",
      " [0.5006412 ]\n",
      " [0.49931288]\n",
      " [0.50010043]] \n",
      " Correct :  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]] \n",
      " Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        sess.run(train, feed_dict = {X:x_data, Y:y_data})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))\n",
    "            \n",
    "        \n",
    "    h,c,a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print('\\n Hypothesis : ', h, \"\\n Correct : \", c, \"\\n Accuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([2,2]), name = 'weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2,1]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7426567 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "100 0.6935443 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "200 0.6908042 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "300 0.6880302 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "400 0.6849305 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "500 0.6812521 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "600 0.6767403 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "700 0.67113173 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "800 0.66416645 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "900 0.65561354 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "1000 0.6453051 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "1100 0.6331595 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "1200 0.6191666 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "1300 0.6033137 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "1400 0.5854548 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "1500 0.5651587 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "1600 0.5416258 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "1700 0.51381963 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "1800 0.48091328 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "1900 0.44291577 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "2000 0.4011116 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "2100 0.3578985 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "2200 0.31597555 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "2300 0.2774497 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "2400 0.24343762 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "2500 0.21418266 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "2600 0.18938172 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "2700 0.16848499 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "2800 0.15088649 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "2900 0.13602069 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "3000 0.12339917 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "3100 0.11261683 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "3200 0.10334436 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "3300 0.09531648 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "3400 0.0883201 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "3500 0.08218396 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "3600 0.076769784 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "3700 0.07196556 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "3800 0.0676799 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "3900 0.06383782 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "4000 0.060377426 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "4100 0.057247393 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "4200 0.05440464 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "4300 0.051813226 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "4400 0.049442694 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "4500 0.047266915 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "4600 0.04526385 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "4700 0.043414474 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "4800 0.041702338 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "4900 0.040113278 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "5000 0.038634896 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "5100 0.037256397 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "5200 0.035968278 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "5300 0.034762185 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "5400 0.033630773 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "5500 0.03256753 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "5600 0.0315666 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "5700 0.030622885 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "5800 0.029731652 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "5900 0.028888786 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "6000 0.02809052 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "6100 0.027333552 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "6200 0.026614793 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "6300 0.025931505 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "6400 0.025281213 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "6500 0.024661537 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "6600 0.024070509 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "6700 0.023506146 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "6800 0.022966847 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "6900 0.022450846 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "7000 0.02195682 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "7100 0.021483384 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "7200 0.02102925 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "7300 0.02059344 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "7400 0.020174677 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "7500 0.01977216 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "7600 0.019384932 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "7700 0.019012118 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "7800 0.018652985 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "7900 0.018306764 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "8000 0.017972797 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "8100 0.017650502 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "8200 0.017339224 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "8300 0.017038483 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "8400 0.016747678 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "8500 0.016466375 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "8600 0.016194174 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "8700 0.015930563 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "8800 0.01567519 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "8900 0.015427655 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "9000 0.015187645 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "9100 0.014954811 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "9200 0.014728857 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "9300 0.014509432 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "9400 0.014296289 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "9500 0.014089178 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "9600 0.013887892 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "9700 0.013692072 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "9800 0.013501658 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "9900 0.013316283 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "10000 0.013135807 [[-1.1778436 ]\n",
      " [ 0.10779107]]\n",
      "\n",
      " Hypothesis :  [[0.0127382]\n",
      " [0.9885795]\n",
      " [0.9827321]\n",
      " [0.01076  ]] \n",
      " Correct :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      " Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = {X:x_data, Y:y_data})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))\n",
    "            \n",
    "        \n",
    "    h,c,a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print('\\n Hypothesis : ', h, \"\\n Correct : \", c, \"\\n Accuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([2,10]), name = 'weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10,1]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.74356145 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "100 0.7024208 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "200 0.68327224 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "300 0.66822135 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "400 0.6518793 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "500 0.6322447 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "600 0.6085079 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "700 0.5801674 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "800 0.54677784 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "900 0.50815886 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "1000 0.46483523 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "1100 0.41829962 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "1200 0.37080806 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "1300 0.32478717 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "1400 0.282215 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "1500 0.24428797 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "1600 0.21142575 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "1700 0.1834766 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "1800 0.15996322 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "1900 0.14027913 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "2000 0.12381075 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "2100 0.110000506 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "2200 0.09837007 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "2300 0.08852221 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "2400 0.08013324 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "2500 0.07294185 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "2600 0.06673777 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "2700 0.061352253 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "2800 0.05664905 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "2900 0.052518148 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "3000 0.04887005 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "3100 0.04563172 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "3200 0.04274319 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "3300 0.040154967 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "3400 0.03782591 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "3500 0.03572152 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "3600 0.03381306 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "3700 0.032076024 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "3800 0.030489843 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "3900 0.029036835 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "4000 0.027701873 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "4100 0.026472032 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "4200 0.025335979 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "4300 0.024283966 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "4400 0.023307545 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "4500 0.022399234 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "4600 0.021552548 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "4700 0.020761773 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "4800 0.020021714 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "4900 0.019327901 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "5000 0.018676413 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "5100 0.01806357 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "5200 0.017486306 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "5300 0.016941652 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "5400 0.016427107 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "5500 0.015940297 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "5600 0.015479149 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "5700 0.015041826 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "5800 0.014626514 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "5900 0.01423176 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "6000 0.01385606 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "6100 0.013498152 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "6200 0.013156869 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "6300 0.012831092 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "6400 0.012519853 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "6500 0.012222189 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "6600 0.011937348 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "6700 0.011664534 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "6800 0.011402909 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "6900 0.011151966 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "7000 0.010911062 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "7100 0.010679589 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "7200 0.010457013 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "7300 0.0102429055 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "7400 0.01003675 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "7500 0.009838164 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "7600 0.009646768 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "7700 0.00946212 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "7800 0.009284006 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "7900 0.009111958 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "8000 0.008945774 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "8100 0.008785122 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "8200 0.008629789 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "8300 0.0084794555 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "8400 0.008333925 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "8500 0.008193015 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "8600 0.008056482 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "8700 0.007924116 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "8800 0.007795778 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "8900 0.0076712277 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "9000 0.0075503867 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "9100 0.007433 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "9200 0.007319035 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "9300 0.0072082505 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "9400 0.007100572 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "9500 0.006995892 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "9600 0.006894029 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "9700 0.0067949537 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "9800 0.0066984836 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "9900 0.006604529 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "10000 0.006513074 [[-1.8676326 ]\n",
      " [ 0.08038494]]\n",
      "\n",
      " Hypothesis :  [[0.00480242]\n",
      " [0.99279577]\n",
      " [0.9941174 ]\n",
      " [0.00807521]] \n",
      " Correct :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      " Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = {X:x_data, Y:y_data})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))\n",
    "            \n",
    "        \n",
    "    h,c,a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print('\\n Hypothesis : ', h, \"\\n Correct : \", c, \"\\n Accuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([2,10]), name = 'weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10,10]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name = 'bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10,10]), name = 'weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = 'bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10,10]), name = 'weight4')\n",
    "b4 = tf.Variable(tf.random_normal([10]), name = 'bias4')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8665265 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "100 0.7113039 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "200 0.69798434 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "300 0.69604254 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "400 0.695364 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "500 0.69486314 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "600 0.69440985 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "700 0.6939832 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "800 0.69357574 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "900 0.6931823 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "1000 0.6927978 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "1100 0.6924182 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "1200 0.69203913 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "1300 0.6916565 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "1400 0.6912662 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "1500 0.6908636 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "1600 0.6904441 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "1700 0.6900023 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "1800 0.6895324 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "1900 0.6890278 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "2000 0.68848073 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "2100 0.68788254 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "2200 0.68722284 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "2300 0.6864898 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "2400 0.68566954 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "2500 0.68474567 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "2600 0.68369883 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "2700 0.6825068 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "2800 0.68114305 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "2900 0.67957723 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "3000 0.67777425 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "3100 0.67569387 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "3200 0.67329156 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "3300 0.6705179 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "3400 0.6673199 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "3500 0.6636423 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "3600 0.65942943 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "3700 0.6546281 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "3800 0.64919007 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "3900 0.64307606 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "4000 0.63625944 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "4100 0.6287301 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "4200 0.6204976 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "4300 0.6115924 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "4400 0.6020619 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "4500 0.59195966 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "4600 0.5813295 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "4700 0.57019216 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "4800 0.55853814 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "4900 0.54632646 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "5000 0.5334833 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "5100 0.51990396 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "5200 0.5054611 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "5300 0.490026 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "5400 0.47350097 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "5500 0.45585805 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "5600 0.43717307 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "5700 0.4176402 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "5800 0.39755484 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "5900 0.37726492 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "6000 0.35710686 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "6100 0.3373532 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "6200 0.31818986 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "6300 0.2997279 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "6400 0.28203747 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "6500 0.2651798 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "6600 0.24921569 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "6700 0.23418942 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "6800 0.22011404 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "6900 0.20697252 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "7000 0.19472799 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "7100 0.18333355 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "7200 0.1727385 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "7300 0.16289201 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "7400 0.15374418 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "7500 0.14524695 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "7600 0.1373544 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "7700 0.13002278 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "7800 0.12321091 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "7900 0.1168801 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "8000 0.11099412 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "8100 0.10551933 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "8200 0.10042437 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "8300 0.09568031 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "8400 0.09126031 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "8500 0.087139644 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "8600 0.08329536 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "8700 0.07970627 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "8800 0.07635279 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "8900 0.07321685 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "9000 0.07028173 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "9100 0.06753199 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "9200 0.06495343 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "9300 0.06253298 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "9400 0.06025859 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "9500 0.058119148 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "9600 0.056104552 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "9700 0.05420544 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "9800 0.052413296 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "9900 0.050720237 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "10000 0.049119122 [[-0.10576171]\n",
      " [-0.7754962 ]]\n",
      "\n",
      " Hypothesis :  [[0.03647444 0.01419112 0.02652678 0.02588564 0.02493796 0.03385159\n",
      "  0.02744016 0.01677161 0.0339486  0.02799541]\n",
      " [0.9473602  0.9447074  0.9459104  0.9344748  0.9590941  0.9651954\n",
      "  0.9392451  0.96211576 0.959235   0.94395435]\n",
      " [0.94992423 0.94123113 0.93945813 0.9327166  0.95559    0.96191704\n",
      "  0.938061   0.95681155 0.95569897 0.9418376 ]\n",
      " [0.05091017 0.08571282 0.07811505 0.09262913 0.04927471 0.01881352\n",
      "  0.08174974 0.04932699 0.04204813 0.06792882]] \n",
      " Correct :  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] \n",
      " Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = {X:x_data, Y:y_data})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))\n",
    "            \n",
    "        \n",
    "    h,c,a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print('\\n Hypothesis : ', h, \"\\n Correct : \", c, \"\\n Accuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
